<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://dafengzai.github.io</id>
    <title>大风仔的博客</title>
    <updated>2021-07-25T14:33:43.973Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://dafengzai.github.io"/>
    <link rel="self" href="https://dafengzai.github.io/atom.xml"/>
    <subtitle>Big brother is watching you</subtitle>
    <logo>https://dafengzai.github.io/images/avatar.png</logo>
    <icon>https://dafengzai.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, 大风仔的博客</rights>
    <entry>
        <title type="html"><![CDATA[Learning-based Congestion Control 介绍]]></title>
        <id>https://dafengzai.github.io/post/ml-on-cc/</id>
        <link href="https://dafengzai.github.io/post/ml-on-cc/">
        </link>
        <updated>2021-07-23T16:41:20.000Z</updated>
        <content type="html"><![CDATA[<p>从TCP/IP协议诞生之日起到今天，拥塞控制问题已经被研究了几十年了，但仍然是一个非常有挑战性的领域，近年来也有许多新的拥塞控制模型的设计被提出，例如Google BBR 与 copa [1].<br>
同时，随着近年来的机器学习热潮，将机器学习的高泛化优势引入CC (congestion control) 的想法也逐渐受到关注，sigcomm20 的 orca [2] 就是一个例子。</p>
<p>那么，在介绍Machine Learing on CC 之前，我们先来谈一谈为什么 ML 可以对CC领域有所帮助。</p>
<h1 id="why-we-need-learning-based-cc">Why we need learning-based CC</h1>
<p>[3] 中提到，由于当前新网络场景与环境的剧烈浮现（数据中心，5G，卫星网络，etc.），使得设计出能够在这些不同场景下都能良好工作的CC机制愈发具有挑战性（效率，公平, etc.）。<br>
同时要注意，在面对复杂多变的网络环境时，一个CC算法往往会会变成 jack of all trades，master of none（杂而不精）或只专注于某类场景 [2]. 例如BBR算法是面对bottleneck buffer bloat的问题而设计，因此在面对浅buffer的场景时，上探带宽引入排队时延的部分在浅buffer场景下就很容易发生丢包而影响最终效果，这种也被称之为 laser-focused solutions.<br>
这也的确是一个必须现实中要面对的问题：CC在建模设计时，都是基于设计者自身思考得出的某种网络假设与建模的。但网络环境的复杂多变决定了设计者在设计CC机制时无法完全覆盖住所有的网络假设与状况。那么这些 modle_based CC which making decidions by predetermined rules 在面对复杂多变的网络状况时，在没有后续人为调整的情况下，是很难一直保持高性能的传输的。</p>
<h1 id="what-is-learning-based-cc">What is learning-based CC</h1>
<p>与传统的基于模型的预先决定好规则的CC不同，learning-based CC 的决策制定是基于<strong>实时</strong>的网络状态的。<br>
这里的 learning-based CC 可以再继续细分为两个部分：最优化性能CC(performance-oriented CC) 与 data-driven/Machine-learning CC. 其中 performance-oriented CC 与 传统CC的边界十分模糊，ML-CC则是比较受到当前CC领域的关注。</p>
<h2 id="performance-oriented-cc">performance-oriented CC：</h2>
<p>这里需要注意，performance-oriented CC 与 传统CC的界限十分模糊，因为这些CC机制的决策规则都是提前就<strong>人为</strong>决定好了的，并且传统CC其实也是<em>实时</em>决策的。若真的要划分出区别的话，则只不过是传统CC机制们是<em>基于事件而非基于状态</em>，例如lose-based CC 基于丢包这一事件修改cwnd，BBR在PROBE_BW状态下上探带宽时依据deliveryRate是否依然保持增长决定下一周期的pacing_gain，根据是否出现 RTT 测量值与 min_RTT 的对比情况判断是否要进入PROBE_RTT状态，这些都是CC基于预先设定好的规则根据网络中是否有目标事件的发生而做出各个传输决策。<br>
而performance-oriented CC 不同，其则是根据预先设定好的模型，通过测量网络中的实时状态（values而非阶梯型的event）来决定下一步的决策。</p>
<p>这里举出两个比较经典的performance-oriented CC 算法：Copa [4] 与 PCC [5]</p>
<ol>
<li>Copa: 决策思路queuing-delay 来计算target_sending_rate. 其最优化目标为：maximize <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>U</mi></mrow><annotation encoding="application/x-tex">U</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>U</mi><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mi>λ</mi><mo>)</mo><mo>−</mo><mi>δ</mi><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mi>d</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">U = log (λ) - δ log (d)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathdefault">λ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03785em;">δ</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathdefault">d</span><span class="mclose">)</span></span></span></span>, where λ = throughput, d = propagation delay.<br>
论文作者在特定的报文到来的模型假设下，便可以求解出在最大化优化目标U时，特定d下的target_sending_rate λ : <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mi>δ</mi><mo>∗</mo><msub><mi>d</mi><mi>q</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">λ = \frac{1}{δ*d_{q}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.3874279999999999em;vertical-align:-0.5423199999999999em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03785em;">δ</span><span class="mbin mtight">∗</span><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16454285714285716em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5423199999999999em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>。并以此为依据将当前sending_rate 往target_sending_rate方向接近。</li>
<li>PCC: PCC也是根据预先设定的计算函数，将当前的网络状态（sending rate, loss rate, RTT）计算出一个 Utility 值，再根据计算出的 Utility值 u 来决定下一步的发送速率。<br>
<img src="https://dafengzai.github.io/post-images/1627185204576.PNG" alt="" loading="lazy"><br>
具体来说：</li>
</ol>
<blockquote>
<p>For each pair, PCC attempts a slightly higher rate r(1+ε) and slightly lower rate r(1 − ε). PCC gets two utility measure ments U+ ,U− corresponding to r(1 + ε),r(1 − ε) re spectively. If the higher rate consistently has higher &gt;utility (U+ &gt; U−), then PCC adjusts its sending rate to r new = r(1+ε);</p>
</blockquote>
<p>但需要注意的是，performance-oriented CC 的决策模型依旧是人为预先设置好的（最优化函数建模, etc.）并基于此规则对实时网络状态做出反应，做出发送决策调整。<br>
而下面将要介绍的 data-driven/Machine-learning CC 则不相同。</p>
<h2 id="machine-learning-cc">Machine Learning CC</h2>
<p>ML CC 与之前的 CC 算法最大的不同就是这些算法会依<strong>据数据</strong>来去更新其模型的参数设置，而不是去依赖认为提前设置好的模型参数，这也是这类CC算法被称之为 data-driven CC 的原因。<br>
具体来说，Machine Learning CC 可以被划分为三类：supervised/unsupervised learning CC 与 Reinforce learning CC</p>
<h3 id="supervisedunsupervised-learning-cc">Supervised/Unsupervised learning CC</h3>
<p>机器学习领域中经典的监督学习与无监督学习在 CC 领域内的相关应用由来已久，早在1999年，就有研究人员将监督学习运用在“拥塞丢包”的原因探究上 [6], 通过报文的 inter-arrival times 用以区分丢包是由于wireless loss 还是 congestion loss. 但对于在 CC 机制本身的设计方面则应用较少（近几年优秀会议上基本没出现）。根本原因还是因为监督/无监督学习是<strong>离线训练</strong>的机制，模型参数的设定是基于离线的训练数据集训练而得来的。虽然可能在测试集上表现优异，但还是跟之前提到过的 traditional CC 的不足一样，无法适应复杂多变的网络环境，一旦真实网络中出现未在训练集中的网络状况时，基于监督/无监督学习的CC机制是无法一直保证高性能的传输的。</p>
<h3 id="reinforce-learning-cc">Reinforce learning CC</h3>
<p>由于传统的监督/无监督的机器学习方法学习在面对复杂多变的网络环境时不能仅依据预先训练好的模型做出足够动态变化的反应以应对，强化学习 Reinforce learning CC 便受到了非常多的关注。<br>
强化学习方法的优势在于：他不仅实时观测实时的网络状态，并且还会对此做出反应，在每一轮内并通过最大化其内 reward function 期望来<strong>利用这些实时的网络状态来调整模型参数</strong>，使之能对网络的变化做出有效反应。这方面的经典工作就是sigcomm20的 Orca [2]</p>
<h4 id="orca-classic-meets-modern">Orca: Classic Meets Modern</h4>
<p>在Orca中，其提出了Machine Learning CC所面对的三个挑战：1- Problem with unseen network scenarios; 2- Convergence issue; 3- Overhead。这些都是挡在ML CC 走向更大舞台所面临的障碍。而Orca创新性的通过“分层”的思想，将 DRL ( Deep Reinforce learning) CC 与 traditional CC 相结合，由 <strong>Deep reinforce learning to decide coarse-grain cwnd as based cwnd.</strong> 下层的传统TCP CC (Cubic) 再在该based-cwnd基础上进行细粒度的cwnd调整，这样就很大程度上解决的之前提到的三个挑战：</p>
<ol>
<li>Problem with unseen network scenarios:<br>
通过强化学习与深度学习相结合，并发挥深度学习从原始数据中提炼高维信息的优势，可以 <em>&quot;utilize  its potentials and capabilities to learn from actual raw input data without relying on preprocessing or handcraft engineering&quot;</em></li>
<li>Convergence issue:<br>
对于收敛性的保证，通过往DRL的数据结果 (based cwnd) 故意加上些扰动，由下层的传统TCP Cubic CC 作后备，来提高Orca的收敛性表现。</li>
<li>Overhead<br>
由于其创新的分层思想，DRL只决定粗时间粒度上的cwnd，下层细时间颗粒度的cwnd调整由Linux内核自带的Cubic执行，客观上降低了计算开销，论文中测试其cpu utilization为不到10%，大幅优于其他learing-based CC 算法。</li>
</ol>
<p>Orca工作时的流程可以参考下图：<br>
<img src="https://dafengzai.github.io/post-images/1627222105961.PNG" alt="" loading="lazy"></p>
<h2 id="next-step">Next Step</h2>
<p>当然，现如今 ML on CC 还有相当大的一段距离要走，其中一个相当重要的问题需要解决，那就是其<strong>可解释性</strong>。一个CC 机制在实际应用时，必须要考虑其收敛性与公平性，而ML CC 可解释性的缺乏使得我们很难去分析其收敛性质与跟其他流竞争时的表现。<br>
sigcomm20 的工作 Metis [7] 给了我们一个思路，将DL-based network systems 解释为决策树与超图 (hypergraphs) 的形式来给管理员们分析DL-based network systems 的各个性能表现提供了帮助。但如何将ML CC 应用到实际，像BBR那样给网络传输性能带来真正帮助，还仍有相当多的工作值得我们去发掘。</p>
<h2 id="参考资料">参考资料：</h2>
<p><a href="https://www.usenix.org/conference/nsdi18/presentation/arun">[1]&quot;Copa: Practical delay-based congestion control for the internet.&quot; 15th {USENIX}  ({NSDI} 18). 2018.</a><br>
<a href="https://dl.acm.org/doi/abs/10.1145/3387514.3405892?casa_token=clEKWQywgrEAAAAA:GtI1V4r5Jmq0k_NotcJxsOy-k17hEyojEZCFPnRtyaiXHQ6vB_BpMu2bmzE8ywjimL2UQ-DU2gyY3w">[2] &quot;Classic meets modern: A pragmatic learning-based congestion control for the Internet.&quot; </a><br>
<a href="https://arxiv.org/pdf/2010.11397.pdf">[3] When Machine Learning Meets Congestion Control: A Survey and Comparison</a><br>
<a href="https://www.usenix.org/conference/nsdi18/presentation/arun">[4] &quot;Copa: Practical delay-based congestion control for the internet.&quot;  ({NSDI} 18). 2018.</a><br>
<a href="https://www.usenix.org/conference/nsdi15/technical-sessions/presentation/dong">[5]  &quot;{PCC}: Re-architecting congestion control for consistent high performance.&quot;  ({NSDI} 15). 2015.</a><br>
<a href="https://ieeexplore.ieee.org/abstract/document/756746">[6] &quot;Discriminating congestion losses from wireless losses using inter-arrival times at the receiver,&quot; 1999 </a><br>
<a href="https://dl.acm.org/doi/abs/10.1145/3387514.3405859">[7]  Interpreting Deep Learning-Based Networking Systems. (SIGCOMM '20).</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[BBR 介绍]]></title>
        <id>https://dafengzai.github.io/post/bbr/</id>
        <link href="https://dafengzai.github.io/post/bbr/">
        </link>
        <updated>2021-06-23T03:55:25.000Z</updated>
        <content type="html"><![CDATA[<p>BBR（Bottleneck Bandwidth and Round-trip propagation time）是Google于2016年提出的一个新的拥塞控制算法，给拥塞控制这一领域提供了一个全新的思路。<br>
传统的拥塞控制算法思路是基于事件（ack，丢包等）来进行发送速率（cwnd）决策的。而BBR则通过测量网络中带宽与RTT两个变量来进行发送速率与方法的决策。</p>
<h2 id="why-we-need-bbr">Why we need BBR</h2>
<p>先说传统的拥塞控制算法的基本情况，传统的CC（congestion control）算法是基于事件反馈来进行决策的。如经典的TCP Reno：在接受到一个成功的ACK时增大cwnd拥塞窗口，在发现丢包时（通常为接受到3个冗余ACK/Duplicate ACKs）执行快速重传与降低cwnd（congestion window）。因此一个理想的TCP Reno算法执行过程为下图（Presentation by Geoff Huston at APRICOT 2018.），为一个锯齿形。<br>
<img src="https://dafengzai.github.io/post-images/1625116498061.PNG" alt="" loading="lazy"></p>
<p>而其后对其进行改进的TCP Cubic，也仅是在增大cwnd是使用非线性函数（cubic函数）而非Reno的线性函数（一般是+1）。因此一个理想的TCP Cubic算法执行过程为下图（Presentation by Geoff Huston at APRICOT 2018.）可以看到与TCP Reno比仍然是一个“锯齿”形的。<br>
<img src="https://dafengzai.github.io/post-images/1625117364252.PNG" alt="" loading="lazy"></p>
<p>那么，这种传统的拥塞控制算法会面临哪些不足呢？Google的BBR论文的论述逻辑为：关注bottleneck！<br>
TCP流量的传输中bottleneck是十分重要的，它不仅决定的网络中的最大发送速率，同时也是队列持续产生的地方（相同入队速率，出队速率最小的地方最容易发送队列堆积）。<br>
当bottleneck的buffer大时，loss-based cc 会倾向于打满它，造成bufferbloat，使得buffer等待时间变长，RTT增大（现在中间节点的buffer相比于之前变得相当大，使得在buffer中的延迟变得重要起来）；当bottleneck的buffer小时，loss-based cc 则会因为浅buffer造成的频繁丢包而使得大多数时间都处在一个较低的发送速率（锯齿形上升时很快就遇到丢包造成cwnd减少）。</p>
<p>接着说回BBR，BBR则采用了另一种不同的思路，即采用RTprop (round-trip propagation time)与BtlBw (bottleneck bandwidth) 来决定传输的速率。见下面这个大家在许多地方都见到过的图：<br>
<img src="https://dafengzai.github.io/post-images/1625197962241.PNG" alt="" loading="lazy"><br>
<strong>BBR的目标，就是去接近上图中的最优点</strong>（optimum operating point）。此时传输过程中的正在发送的inflight报文数量正好匹配bottleneck的处理速率（inflight报文数少了会造成带宽浪费，多了会超过bottleneck处理速率造成buffer堆积使得RTT增大）。该最优点对应的传送报文数 inflight =<em>BDP</em>* (bandwidth-delay product).  此处inflight为发送但未接收到对应ACKs的报文数</p>
<pre><code>BDP = BtlBw × RTprop
RTprop = 一段时间内(数十秒)的minRTT
BtlBw = 一段时间窗口(6-10RTTs)内最大的deliverayRate(Δdelivered/Δt)
</code></pre>
<p>因此，BBR算法的工作流程就很清晰明了了：通过测量链路中的BtlBw与RTprop来计算BDP，再通过调整发送端发送速率使得inflight的报文等于BDP.</p>
<h2 id="bbr-architecture">BBR Architecture</h2>
<p>知道了BBR算法的设计思路后，在实现的过程中必然会遇到这两个问题：如何测量当前链路的BDP与如何调整发送速率使得inflight packets匹配BDP。针对第一个问题，BBR与各类CC类似，在接收到报文ACK时统计其 RTT 与 deliveray rate ，并以此更新RTprop与BtlBw. 而对于第二个问题，BBR选择采用cwnd_gain与pacing机制一同来调整发送速率，具体代码如下：</p>
<pre><code>function send(packet)
    bdp = BtlBwFilter.currentMax RTpropFilter.currentMin // BDP 估计
    if (inflight &gt;= cwnd_gain*bdp)  // 可以看作是传统CC 中的 cwnd 
        // wait for ack or timeout
        return
    if (now &gt;= nextSendTime)
        packet = nextPacketToSend()
        if (! packet)
            app_limited_until = inflight
            return
        packet.app_limited = (app_limited_until &gt; 0)
        packet.sendtime = now
        packet.delivered = delivered
        packet.delivered_time = delivered_time
        ship(packet)
        nextSendTime = now + packet.size /  (pacing_gain * BtlBwFilter.currentMax)
    timerCallbackAt(send, nextSendTime)
</code></pre>
<p>可以看到BBR发送的目标也是保证inflight的报文不会超过当前链路的BDP，因此每个packet进行发送时都会进行处理。<br>
当有新的报文需要发送时，BBR仅会当inflight的报文小于BDP<em>cwnd_gain</em>（比例系数，后面会详谈）进行发送。同时发送动作也是由pacing机制控制着，目的是防止突发的发送速率增高超过bottleneck的处理速率造成排队，因此pacing速率也是基于当前带宽bandwidth的。BBR pacing机制具体来说为只有 now &gt;= nextSendTime 时才执行发包动作，且将数据包在时机尺度上尽量平均送出(nextSendTime = now + packet.size /  (pacing_gain * BtlBwFilter.currentMax)) 此处pacing_gain为控制pacing rate的增益系数，数值越大则数据包的发送越密集，为1时则是等待上个数据包传输完成后再发送。</p>
<h3 id="bdp-的估计">BDP 的估计</h3>
<p>看完了数据包发送时的处理流程，下面就该介绍BBR最为核心的部分：<strong>BDP的估计</strong>。<br>
其实通过带宽（BtlBw）与延时（RTprop）来判断链路状态的想法很早就有了。上面optimum operating point的图就是基于Leonard Kleinrock 的工作，其于1979年论述了该operating point就是拥塞控制算法能达到的最佳的点。但为何直到最近才随着BBR得到大规模的应用呢？原有是同时代Jeffrey M. Jaffe 证明了使用某种算法去收敛到该operating point是不可能的（RTprop的测量需要传输链路上没有排队，而没有排队就无法判断当前发送速率是否达到了瓶颈带宽BtlBw，因此二者无法同时测得），因此在该方向上的研究一直没能掀起什么大的风浪。</p>
<p>再说回BBR，既然使用某种算法去收敛到最优点是不可能的，那么BBR基于带宽（BtlBw）与延时（RTprop）计算得来的BDP = BtlBw*RTprop是怎么保证能够尽可能接近该最优点optimum operating point的呢？</p>
<p>BBR的做法是通过探测一段时间内的网络状况来去估计网络的RTTprop与BtlBw：</p>
<pre><code>Although it is impossible to disambiguate any single measurement, a connection’s behavior over time tells a clearer story,
</code></pre>
<p>BBR的做法是设计两个不同的状态去分别估计RTTprop与BtlBw。这样设计的合理性是<strong>网络层路由选择算法更新传输路径(OSPF中链路状态变化或周期触发, etc.)的变化频率相较于cc决策来说低很多</strong>（分钟/秒级别 vs RTT毫秒级别）。这就意味这在cc的很多轮cwnd决策过程中，下边网络层的传输路径是不变的，同时报文在上面传输的RTTprop也是不变的。<br>
既然RTTprop变化频率很低，因此BBR大部分时间都处在预测BtlBw的状态中（probeBW）。由于当达到最大瓶颈带宽/BtlBw时，再次增加发送带宽会造成排队使得RTT开始增长（隐含前提是这段时间内RTTprop不变），因此在处于probeBW状态时BBR通过周期性上探发送带宽（配合以带宽下降用以排空上探造成的堆积的队列），若上探到某一带宽后继续上探时RTT开始增长，就意味着该带宽就是BtlBw。<br>
而对于RTTprop的估计，若一段时间（10s）内没有接收到小于等于RTTprop的RTT的ack，就认为传输路径可能发生了变化，进入probeRTT状态去排空队列并重新计算RTTprop。（在与尝试打满buffer的loss-based流共存时如何得到可信的RTTprop估计是BBR设计里最容易被挑战的一点，之后的文章再详谈）</p>
<h3 id="bbr-states">BBR States</h3>
<p>再说回到BBR的具体工作流程，下面就是BBR算法的状态转移情况（copy from: github.com/google tcp_bbr2.c）</p>
<pre><code>Here is a state transition diagram for BBR:

             |
             V
    +---&gt; STARTUP  ----+
    |        |         |
    |        V         |
    |      DRAIN   ----+
    |        |         |
    |        V         |
    +---&gt; PROBE_BW ----+
    |      ^    |      |
    |      |    |      |
    |      +----+      |
    |                  |
    +---- PROBE_RTT &lt;--+
</code></pre>
<p>需要注意的是：BBR 的各个状态仅仅决定pacing_gain 与 cwnd_gain 的取值。各个状态的取值为：</p>
<blockquote>
<p>STARTUP: pacing_gain 2.89，cwnd_gain 2.89<br>
DRAIN: pacing_gain 0.35, cwnd_gain 2.89<br>
PROPEBW：pacing_gain [5/4, 3/4, 1, 1, 1, 1], cwnd_gain 2<br>
PROPERTT: pacing_gain 1 cwnd_gain 1(配合以cwnd固定大小=4)</p>
</blockquote>
<p>结合上边的发送过程</p>
<pre><code>if (inflight &gt;= cwnd_gain*bdp) 
        // wait for ack or timeout
        return
'''
nextSendTime = now + packet.size /  (pacing_gain*BtlBwFilter.currentMax)
</code></pre>
<p>cwnd 决定可以往链路中发送的容量，数值越大代表着可以往链路中发送的报文越多；<br>
pacing_gain为控制pacing rate的增益系数，数值越大则数据包的发送越密集 (burst sending).<br>
可以看到，BBR 对 inflight packets的调整是通过调整pacing速率来进行的。cwnd_gain 的变化基本是为了配合pacing_gain的变化而调整。</p>
<h3 id="bbr-状态转移过程">BBR 状态转移过程</h3>
<p>对于状态变化的具体情况：BBR首先会从STARTUP状态开始，并尽快的提升其的发送速率（指数级）</p>
<blockquote>
<p>Startup implements a binary search for BtlBw by using a gain of 2/ln(2) = 2.89 to double the sending rate while delivery rate is increasing.<br>
This discovers BtlBw in <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">log_{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>(BDP) RTTs but creates up to 2BDP excess queue in the process.</p>
</blockquote>
<p>此时 pacing_gain = cwnd_gain = 2/ln(2) = 2.89. 发送速率2倍于BDP.</p>
<p>当BBR判断传输链路变满时 (通常STARTUP状态下BtlBw三轮涨幅不超过 25%)，可能会在传输链路上堆积up to 2BDP excess queue in the process (2倍往上递增)，因此需要进入DRAIN排空阶段来排空这些堆积，直到 inflight 降到 BDP 为止。</p>
<blockquote>
<p>The pacing gain of 1/high_gain in BBR_DRAIN is calculated to typically drain the queue created in BBR_STARTUP in a single round</p>
</blockquote>
<p>进入DRAIN阶段后：</p>
<pre><code>pacing_gain = 1/high_gain = 1/(2/ln(2)) = 0.35;  // 选择降低发包密度（pacing slow）
cwnd_gain = bbr_high_gain = 2.89. // 此时BBR选择维持 cwnd
</code></pre>
<p>可以看到 BBR 在 Drain 状态选择维持 cwnd 大小而降低 pacing rate，通过pacing机制降低发包的速率以此来减少 inflight 的报文。当 inflight packets &lt;= BDP 时，BBR 进入稳定阶段（steady-state），这个阶段只会在PROBE_BW与PROBE_RTT之间切换。</p>
<p>BBR 会首先从 Drain 切换到 PROBE_BW 状态，一个BBR流大部分时间都是重复着PROBE_BW这一状态去探测传输管道的可用带宽。此时其通过 gain cycling 的机制使得pacing_gain顺序的在[5/4，3/4，1，1，1，1，1，1]中进行选择。而pacing_gain 是否进入下一个值则依赖于inflight 报文是否仍处于当前值期望的状态，如pacing_gain=5/4时，BBR上探BtlBw，若此时 deliveryRate 仍增长（Δdelivered增大且此时Δt不相应变大，代表bottleneck还未堆积队列造成RTT增长），则pacing_gain 仍等于 5/4，否则则进入下一个取值 pacing_gain = 3/4 排空inflight报文。</p>
<p>同时，如果在10s内未出现相等于BBR的min_RTT的值（或出现小于RTTprop的值），BBR就认为链路中的传播时延RTTprop发生了变化，进入PROBE_RTT状态去探测最新的RTTprop。方法是去将inflight的报文降到最低，此时设置cwnd=4，当inflight &lt;= 4时开始重新探测min_RTT来作为RTTprop的值。在维持PROBE_RTT最少200ms后，BBR会根据管道是否被打满去选择进入PROBE_BW或STARTUP状态。</p>
<p>一个典型的 BBR 流开始周期可以参考下图：<br>
<img src="https://dafengzai.github.io/post-images/1626104649755.PNG" alt="" loading="lazy"></p>
<blockquote>
<p>Red: CUBIC sender; Grean: BBR sender; Blue: ACK</p>
</blockquote>
<h3 id="bbr-多流">BBR 多流</h3>
<p>多个 TCP 流场景下可以再细分为两种情况：多条BBR流与多条混合的流。</p>
<p>对于多条BBR流的场景，公平性的保证是依靠BBR处于大部分处于的PROBE_BW状态的周期性带宽让出与PROB_RTT状态下流发送速率降低到最低限度来保证的。</p>
<p>具体来说，PROBE_BW状态下pacing_gain会周期性的到达3/4=0.75的值，这将降低BBR sender发送报文的速率，以将带宽让渡给其他流。但这里需要注意的是，pacing_gain取0.75时意味着其在上一个周期的pacing_gain值为5/4=1.25（密集发包），因此其上一个周期内上探带宽的动作其实也挤占了更多的流。当前周期让渡出的带宽目的是为了排空上探带宽时造成的堆积报文，并不能完全保证公平性。<br>
因此公平性的保证主要的是靠large flows进入PROBE_RTT状态，将发送窗口降低到4来极大地降低网络中传输地数据量，而网络中传输数据的大量降低通常会导致排队时延减低，这样其他BBR流就会探测到更低的RTT值，并且也一同进入PROBE_RTT状态，发送窗口设为4让出带宽，使得传输时延进一步降低，其他流也探测到了更低的RTT值... 以此让这些BBR流一齐进入PROBE_RTT状态，并在PROBE_RTT状态结束后一齐依据BBR算法探测BDP从而决定发送带宽（下图30sec时一齐进入PROBE_RTT带宽降低，之后一齐上探从而使得带宽公平利用）。这种分布式的协助机制是BBR流之间保证公平性的关键。<br>
<img src="https://dafengzai.github.io/post-images/1626501478820.PNG" alt="" loading="lazy"></p>
<p>然而，对于非BBR流之间的公平性保证，效果就不一定好了。从上段可以看出，不同BBR流分布式的配合协助机制生效的前提是：large BBR flows会周期性进入PROBE_RTT状态让出带宽，并且让出带宽这个动作造成的延时下降使得其他的flows探测到更低的RTT。<br>
但在面对其他类型的flows时（例如loss-based 的 Reno/Cubic），若占据大量带宽的是 loss-based flow，那么周期性进入低发送速率状态这一前提就不存在了。只有在碰上丢包事件时才会降低发送速率，而从前面的介绍中可以判断此时bottleneck的排队情况已经很严重了（发送丢包）,已经影响到时延敏感的flow的体验。同时，BBR流通过探测到新的minRTT值来进入同步PROBE_RTT状态再一齐上探保证公平性时，其他不基于RTT作发送判断的流则不会下调其发送带宽，去抢占了更多BBR flows出让的带宽，这就使得BBR的公平性保证受到了挑战。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[TCP 传输机制介绍]]></title>
        <id>https://dafengzai.github.io/post/tcp-ack-ji-zhi/</id>
        <link href="https://dafengzai.github.io/post/tcp-ack-ji-zhi/">
        </link>
        <updated>2021-04-12T14:31:04.000Z</updated>
        <content type="html"><![CDATA[<p>##TCP 简单介绍</p>
<p>TCP 是因特传输层的一个面向连接的可靠传输协议，用以一个主机的应用程序向另一主机的应用程序发送数据。因为其是面向连接的，因此在发送数据之前，两个进程必须先“握手”，建立确保可靠传输的相关参数。</p>
<h2 id="tcp-的连接建立过程">TCP 的连接建立过程</h2>
<p>TCP 的连接建立被称之为“三次握手”是因为其在建立连接时在两个主机之间发送了3个报文段。并且在连接建立时可通过TCP报头设置相关参数，如首部长度、接收窗口字段（愿意接收的字节数）, etc.<br>
因为TCP建立的是可靠传输，因此在连接建立与发送时离不开的重要字段就是<strong>序号字段</strong>与确<strong>认号字段</strong>。TCP将传输的数据看成是一个有序的字节流，因此序号字段为该segment首字节的字节流编号，同时由于TCP是全双工的，还可能会接收到数据，因此确认号设置为期望收到的下一字节的序号。</p>
<p>具体来说，<strong>TCP连接建立的过程</strong>为：<br>
<img src="https://dafengzai.github.io/post-images/1626797945219.PNG" alt="" loading="lazy"><br>
capture from <a href="https://www.eet-china.com/mp/a44399.html">一口Linux</a><br>
第一步：<em>SYN</em>，发送方发送SYN标志位置1的TCP报文段，并随即地选择一个初始序号。<br>
第二步：<em>SYNACK</em>， 接收方从提取出TCP SYN报文段，为该TCP连接分配TCP缓存与变量。最后，接收方也会选择自己的初始序号。<br>
第三步：<em>连接建立</em>，发送方收到SYNACK报文段后，也需要为该链接分配缓存与变量，将SYN标志位置0表示连接已建立，此时TCP报文段可以负载数据进行发送了。</p>
<p>而对于<strong>TCP连接终止过程</strong>：则是通过连接的任意一方发送终止报文（FIN标准位置1），另一方返回FINACK，并在自身的数据传输完成后也发送一个FIN报文。在接收到终止报文的确认后，两边对于该连接的所有资源都被释放了。<br>
<img src="https://dafengzai.github.io/post-images/1626798691039.PNG" alt="" loading="lazy"><br>
capture from <a href="https://www.eet-china.com/mp/a44399.html">一口Linux</a></p>
<p>这里在顺带提一下为什么需要“三次握手”：因为TCP建立的是<strong>双向</strong>（全双工）的可靠连接，因此在接收到一个方向的连接建立请求时需要返回一个确认信息（ACK），只不过在客户进程向服务进程发送连接建立请求时，服务进程的连接建立请求信息可以放在确认信息报文里一同发送。因此两端的两个连接建立报文发送过程可以被简化为三次（三次握手）。<br>
而对于四次挥手，由于一方发送终止请求，另一个确认后，另一方仍可能有数据需要传输，因此得等到另一方数据传输完毕后才能发送终止请求。对其确认后两边都再没有数据需要发送，便可以是否该连接的相关资源。</p>
<p>另外，当TCP服务器接收一TCP SYN报文，但该主机对应端口不接受连接时，会向源主机发送RST标志位置1的特殊重置报文，以告诉源主机不要再继续发送了。</p>
<h3 id="tcp-连接的安全性">TCP 连接的安全性</h3>
<p>TCP 连接建立时面临一个安全威胁，即SYN泛洪攻击，攻击者发送大量SYN报文而不进行三次握手的后续步骤，导致服务器不断为这些半开连接分配资源造成浪费甚至消耗殆尽。为应对该攻击，SYN cookie机制应用而生。<br>
SYN cookie工作方式为：<br>
当接收到SYN报文段时，并不会为此消耗资源来生成一个半开连接，而是根据SYN报文用户端的相关信息与服务器单独知道的秘密数来生成一个初始TCP序列号（cookie），因此服务器发送的时带有cookie作初始序列号的SYNACK分组。<br>
由于服务器不维护任何cookie与对于SYN报文的信息，不分配任何资源，因此避免了SYN防洪攻击的危害。<br>
而对于正常的TCP连接，返回cookie对应的ACK时（ACK_seq = cookie + 1）再根据报文内相关信息（与SYN报文段内一致）与秘密数再生成一次cookie，若该cookie + 1 = ACK_seq，则认为是合法的。服务器生成一个全开的TCP连接。</p>
<h2 id="tcp-发送接收">TCP 发送/接收</h2>
<p>在上层将数据交付给TCP进行发送时，TCP会将数据引导到该对应连接的发送缓冲（send buffer）。接着TCP就以报文/Segment为单位将数据交付给下层的网络层进行发送。segment大小受到<strong>最大报文长度MSS</strong>（Maximum Segment Size）约束，与链路层最大传输单元MTU相关，一般为MTU(1500Bytes) - TCP/IP Header(normally 20+20 = 40Bytes) = 1460Bytes.<br>
同理，当TCP在另一端接收到一个segment后，便将其内数据放入改TCP连接的接收缓存（recieve buffer）中，上层应用程序便可以从此缓存中读取数据流。</p>
<h2 id="tcp-确认ack机制">TCP 确认(ACK)机制</h2>
<p>TCP提供的是一个可靠传输，确保进程从TCP接收缓存中读取到的数据流是正确、按序的字节流，即需要与发送端发送的字节流相同。<br>
为了保证数据报的正确传输，需要对对于报文的送达情况作确认（接收方发送ACK报文给发送方）。TCP提供的是<em>累积确认</em>的机制，即只确认该流中正确接收到（至第一个丢失字节为止）的字节。<br>
对于ACK的产生，RFC5681建议：</p>
<table>
<thead>
<tr>
<th>事件</th>
<th>接收方ACK操作</th>
</tr>
</thead>
<tbody>
<tr>
<td>当具有期望序号的按序报文到达</td>
<td>正确接收字节变化，ACK变化，ACK报文等待传输，但此时会进行delayed动作，最多等待500ms，若下一个具有期望序列号的报文segment到达，ACK变化，立即发送累计ACK，以确认这两个按序报文段。</td>
</tr>
<tr>
<td>当比期望序号大的失序报文到达</td>
<td>接收字节流产生间隔，立即发送冗余ACK，其确认字段号为间隔低端的序列号</td>
</tr>
<tr>
<td>当能填补接收字节流间隔的报文到达</td>
<td>若该报文确认号为间隔的低端，则立即发送ACK</td>
</tr>
</tbody>
</table>
<p>TCP通过ACK确认机制来对报文的各个发送场景进行处理：正确送达、丢失和失序。<br>
对于正确送达的报文，正常地返回ACK报文进行确认即可，而对于未正常抵达的报文，通常有两种情况：丢失与失序。下面将介绍对于这两种情况地处理：<br>
对于<strong>丢失</strong>报文的处理，其采用”超时/重传“机制来进行处理。那么就面临一个问题：超时的阈值该如何设定，依据RFC6298关于管理TCP定时器的建议。可以仅在某个时刻做一次SampleRTT的测量，并依据指数加权移动平均EWMA估计一个EstimatedRTT与波动情况DevRTT。这样超时的阈值便为</p>
<pre><code>TimeoutInterval = EstimatedRTT + 4*DevRTT
</code></pre>
<p>当然，这只是建议，在实际部署中各个服务商都可以根据自己的需求来设置超时阈值。</p>
<p>超时重传机制存在一个明显的问题，只有在超过超时阈值时才进行丢包重传的处理，增加了端到端的延时，因此TCP设计了冗余ACK (duplicate ACK)机制，冗余ACK的产生意味着接收方探测到了数据流中地一个间隔，可能是由于报文丢失或乱序抵达。由于TCP的累积确认机制，其只得对已接收到的最后一个按序字节数据进行确认（冗余ACK），若TCP接收方收到对相同数据地3个冗余ACK，便可以将其当作一种指示，仍未发生 了丢包，进行快速重传。</p>
<p>同时，对于这些失序到达的报文，TCP一般选择先缓存保留失序的字节，并等待缺少的字节送达以填补缺失的间隔。</p>
<h3 id="sack">SACK</h3>
<p>更进一步地思考，当TCP发送端进行快速重传时，到底应该发送多少个报文呢？这就要引入SACK (selective acknowledgement) [RFC2018] 机制了。SACK数据报会在ACK报文的基础上，再添加最近接收到的序列号的范围，这样TCP发送端就会知道接收端的数据接收情况，知道缺失了哪些数据块，重传时就可以确定需要重新发送的数据范围了。<br>
<img src="https://dafengzai.github.io/post-images/1626884923965.PNG" alt="" loading="lazy"><br>
capture from <a href="https://www.jianshu.com/p/f7f75a0f6384l">TCP 可靠传输的实现（二）TCP的重传机制</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[TCP/IP协议中的网络层（概述）]]></title>
        <id>https://dafengzai.github.io/post/tcpip-xie-yi-zhong-de-wang-luo-ceng-gai-shu/</id>
        <link href="https://dafengzai.github.io/post/tcpip-xie-yi-zhong-de-wang-luo-ceng-gai-shu/">
        </link>
        <updated>2021-03-24T16:20:55.000Z</updated>
        <content type="html"><![CDATA[<p>本篇开始关注网路协议中的”网络层“，TCP/IP协议中在网络层中采用标准化的IP协议。网络层的主要任务就是就是通过路由选择算法，为数据报分组通过通信子网选择最合适的路径，将将数据报分组及时送达。</p>
<p>具体来说，网络层的功能有三点：</p>
<ol>
<li><strong>路由选择与分组转发</strong>：通过各种路由算法，为数据报通过通信子网选择最合适的转发路径。</li>
<li>异构网络互联：IP 协议屏蔽了下层物理网络的差异，为上层提供统一的 IP 数据报。</li>
<li>拥塞控制：防止所有节点来不及接收分组而造成大量丢弃的机制。</li>
</ol>
<p>本篇仅作网络层的基本介绍，后续有关路由选择的具体介绍见后续文章。</p>
<h2 id="ip-数据报结构">IP 数据报结构</h2>
<h3 id="ip-报文头">IP 报文头</h3>
<p><strong>IPv4</strong></p>
<pre><code>RFC 971 defines the fields of the IPv4 header on page 11 using the following
diagram: (&quot;Figure 4&quot;)
   0                   1                   2                   3
   0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
  +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
  |Version|  IHL  |Type of Service|          Total Length         |
  +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
  |         Identification        |Flags|      Fragment Offset    |
  +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
  |  Time to Live |    Protocol   |         Header Checksum       |
  +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
  |                       Source Address                          |
  +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
  |                    Destination Address                        |
  +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
  |                    Options                    |    Padding    |
  +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
</code></pre>
<p>主要关注的几个字段有：</p>
<ul>
<li>总长度：首部+后接的数据部分，单位是字节B</li>
<li>区分服务：指示期望获得哪种类型的服务</li>
<li>生存时间（TTL）：IP分组的保质期。经过一个路由器-1，变成0则丢弃</li>
<li>协议：数据部分的协议</li>
<li>首部检验和：只检验首部</li>
<li>源IP地址和目的IP地址：32位</li>
</ul>
<p><strong>IPv6</strong></p>
<pre><code>RFC2460 section 3
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |Version| Traffic Class |               Flow Label              |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |         Payload Length        |  Next Header  |   Hop Limit   |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |                                                               |
    +                                                               +
    |                                                               |
    +                         Source Address                        +
    |                                                               |
    +                                                               +
    |                                                               |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |                                                               |
    +                                                               +
    |                                                               |
    +                      Destination Address                      +
    |                                                               |
    +                                                               +
    |                                                               |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
</code></pre>
<p>值得关注的有：</p>
<ul>
<li>Next Header: Pv6报文是由IPv6标准头+扩展头+负载Payload组成，Next Header字段就指定了扩展头的类型</li>
<li>Payload Length: IPv6 报文头之后的数据长度，单位是字节</li>
<li>Hop Limit: 与IPv4 报文头中的Time to Live类似，每经过一个转发节点减一，为0便丢弃。</li>
<li>128 bit的IPv6地址</li>
</ul>
<h2 id="分组转发">分组转发</h2>
<p>转发（forwarding）一般指网络中的路由器转发IP分组的物理传输过程与数据报转发机制，用于（根据路由表/转发表FIB）决定数据报分组从哪个路径（端口）发送出去。至于如何决定哪些IP地址的报文该从哪个端口进行转发，则是由路由算法进行决定的。</p>
<h2 id="拥塞控制">拥塞控制</h2>
<p>拥塞，在网络中的各个层都会发生。在网络层上，节点来不及接收分组便会发生拥塞。拥塞控制机制主要关心在传输超过网络能够在合理的报文传输延时范围内传输的数据量时发生的情况，是一个网络上的全局性的问题，包含网络中的各个主机与路由器。在网络层上亦有负载均衡/流量调节的机制来防止数据报分组的转发发生拥塞。</p>
<p>值得注意的时，在网络层上的拥塞控制主要关注的是“路径”，在某个链路负载大时通过降低源节点的发送速率（流量调节）或将流量引导到其他链路转发到目标节点（负载均衡），从而保证传送的数据不超过链路的容量。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[TCP/IP协议概览（网络接口层）]]></title>
        <id>https://dafengzai.github.io/post/tcpip-xie-yi-jie-xi/</id>
        <link href="https://dafengzai.github.io/post/tcpip-xie-yi-jie-xi/">
        </link>
        <updated>2020-11-04T15:28:15.000Z</updated>
        <content type="html"><![CDATA[<h1 id="总览">总览</h1>
<h2 id="tcpip协议总览">TCP/IP协议总览</h2>
<p>TCP/IP协议主要由：数据链路层、网络层、传输层、应用层这几部分组成。本章主要关注数据链路层</p>
<h1 id="网络接口层">网络接口层</h1>
<p>网络接口层可以初略对应OSI七层模型中的物理层与数据链路层，夹在网络层与下面的物理层之间，主要工作是对网络层到来的IP数据包封装成数据链路层的“数据帧”，并将数据帧通过通信设备（一般为网卡）送入物理层中通过传输介质传输，并对在物理层传输的比特流进行<strong>差错控制与流量控制</strong>，将物理层提供的可能出错的物理连接改造成为逻辑上无差错的数据链路，使之对网络层表现为一条无差错的链路。</p>
<h2 id="差错控制">差错控制</h2>
<p>比特流在传输介质中传输时，由于线路本身产生的噪声与外界因素的影响，使得比特流中的某些比特位发生了错误，由1变成0，或由0变成1。或者实在数据帧层面上发生了丢失/重复/乱序。这些错误都会影响链路层向上为网络层提供可靠服务。</p>
<p>对于比特位发生的错误，在数据链路层上的基本原则都是通过冗余编码来对一组二进制比特串进行检错或纠错，即对应着数据链路层的检错编码（常用的例如CRC循环冗余编码）与纠错编码（例如海明编码），显然纠错编码需要的冗余度要大于检错编码，使用哪种编码方式便取决于信道的可靠控制，当信道非常可靠的时候，检错码代价就会小于纠错码。</p>
<p>数据链路层的数据帧尾部的FCS一般就是用于差错控制的比特位。</p>
<h2 id="流量控制">流量控制</h2>
<p>与传输层类似，数据链路层上也有自身的流量控制机制，但区别在于传输层控制的是端到端之间的数据流量，而数据链路层上的流量控制是控制相邻两个节点之间的，仅为了保证接收方节点有足够的缓存空间来接收流量数据。</p>
<p>在数据链路层上的流量控制机制有：</p>
<ol>
<li>停止等待协议：在收到确认后再发送下一个数据帧，简单，但信道利用率低。</li>
<li>滑动窗口协议：有后退N帧协议（GBN）：发送窗口大小＞1，接收窗口大小=1，采用累积确认的方式 /  选择重传协议（SR）：发送窗口大小&gt;1, 接收窗口大小&gt;1，不管乱序，正确接收就返回ack，同时失序帧被缓存。</li>
</ol>
<h2 id="组帧">组帧</h2>
<p>在数据链路层上传输的单位是帧，数据链路层的第一项主要任务就是将网络层送来的IP数据包封装成“帧”，其所能容纳的最大数据长度也叫最大传输单元（MTU）一般默认是1500比特。封装成帧的方式一般就是在一段数据的前后分别添加头部与尾部用于标记（一般为特定比特串，例如01111110(HDLC协议，同时数据内连续5个1后添0)），方便从接收端收到物理层上交的比特流中识别数据帧的开始与结束。<br>
<img src="https://dafengzai.github.io/post-images/1615305313242.png" alt="" loading="lazy"></p>
<p>同时这些数据帧都是在接入了网络的网络适配器（网卡）之间进行传输，而网卡地址（MAC地址）就是数据帧的发送地址和接收地址。有了MAC地址以后，以太网采用广播形式，把数据帧发给所在子网络内所有主机，子网内每台主机在接收到这个包以后，都会读取首部里的目标MAC地址，然后和自己的MAC地址进行对比，如果相同就做下一步处理，如果不同，就丢弃这个包，以此来实现数据帧在相邻节点间的传输。</p>
<p>网络层的基于IP地址的报文转发要在数据链路层上转换为基于MAC地址的数据帧转发，还需要引入一个”ARP“协议来将目标端的IP地址转换为为对应的MAC地址进发送。具体来说就是发送时主机依靠IP-MAC的对应关系/ARP表来封装数据帧，在数据帧头部封装上对应的MAC地址，而二层设备在收到含有目标MAC地址的数据帧后，查看自己的“MAC地址表”，便知道这个MAC地址该从自己的哪个端口发送出去。以此将数据帧送到相应的目标节点。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[WSL2：Linux下开发的最好入门工具]]></title>
        <id>https://dafengzai.github.io/post/wsl2linux-xia-kai-fa-de-zui-hao-ru-men-gong-ju/</id>
        <link href="https://dafengzai.github.io/post/wsl2linux-xia-kai-fa-de-zui-hao-ru-men-gong-ju/">
        </link>
        <updated>2020-10-20T15:44:27.000Z</updated>
        <content type="html"><![CDATA[<h2 id="wsl2介绍你为什么需要他">WSL2介绍：你为什么需要他</h2>
<p>很多人接触编程一开始都是在Windows电脑上使用visual studio开始第一个C语言的程序，但“编程”这一词语包含着许多的意义，在Windows上编写C语言程序只是其中的一部分，当你想见识这广阔的编程学习世界时，就会发现仅在Windows下进行编程有着很大的局限性。</p>
<p>如果你是一名网络工程师，那么Linux下的开发就会是你的一项必备技能，不仅如此，像Java、python、go等语言都能够在Linux下获得很好的开发体验。Linux相比与Windows，除了开源这一无与伦比的巨大优势外，还有着诸如apt包管理工具（让你一步配置好环境）、环境配置方便、开源平台下无数前人帮你踩的坑，造的轮子，这些都会成为你学习路上的巨大助力。</p>
<p>无论你是一时兴起或是学习需要，假设你现在需要在Linux下进行开发。并且你和我一样，不舍得Windows平台下这些方便的图形界面工具，那么你就需要在Windows下进行Linux开发的方法。在Windows下通过终端或开发工具在Linux下开发无外乎这三种方法：</p>
<ol>
<li>租一个Linux服务器，并在之上进行开发。这可以说是最好的方法，如果你条件允许的话我最推荐这一种方法。</li>
<li>Windows下使用虚拟机安装Linux镜像进行开发。首先这其实也是有成本的，使用虚拟机需要你允许在Windows专业版系统之上，且专业的虚拟机软件也是需要收费的。从某种意义上来说WSL2也是一个拥有完整Linux精选的虚拟机，只是我认为他更方便，对用户跟友好而已。</li>
<li>使用WSL2，即Windows subsystem for Linux2，微软在开源领域发力的一大力作，在window10 2004版本起正式启用，无论是专业版还是家庭版的win10，都可以开启这一功能来免费体验Linux下开发的快捷。使用WSL2能够获得接近原生Linux的体验，又能学习Linux命令又能保留Windows下的日常使用的便捷。何乐而不为？</li>
</ol>
<h2 id="wsl2的安装">WSL2的安装</h2>
<p>首先建议各位在Microsoft store上安装WindowsTerminal，Windows下最好的命令行工具。之后要确认本机的Windows系统是否为2004及以上。首先可以在设置—&gt;系统—&gt;关于—&gt;Windows规格处可查看到windows的版本号，只需大于等于2004即可，如果版本号小于2004，可参考<a href="https://www.microsoft.com/zh-cn/software-download/windows10">Windows官网</a>下载官方升级工具来在保存电脑文件的基础上升级到2004版本。</p>
<p>之后建议按照官网的<a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10">步骤</a>来一步步配置，官网的信息与安装教程时最新最权威的，比啥CSDN博客啥的好多了，建议装个有道词典扩展来辅助阅读网页。</p>
<p>注意：</p>
<ol>
<li>wsl --set-default-version 2 命令处可能会提示内核不是最新版，只需安装提示网站下载最新内核安装即可。</li>
<li>在启动Ubuntu界面时，可能会出现一个错误：<img src="https://dafengzai.github.io/post-images/1603204308746.png" alt="" loading="lazy"></li>
</ol>
<p>这是因为WSL2是基于Hyper-V 运行的，此时只需在BIOS中启用虚拟化即可（我的华硕主板为BIOS—&gt;高级设置—&gt;cpu设置—&gt;启动Intel虚拟化设置）</p>
<h2 id="wsl2下进行开发">WSL2下进行开发</h2>
<h3 id="open-vs-code-in-wsl-2">Open VS Code in WSL 2</h3>
<p>vscode是微软发布的一个文本编辑器，在各路插件的支持下，他已经是一个相当强大的编程工具了，再配合上remote-wsl插件，可以说是WSL2的一个必备伴侣了。</p>
<p>下面我就就建议在Windows下安装好vscode，并下载启用Remote Development来调用WSL2。<br>
<img src="https://dafengzai.github.io/post-images/1603204712394.png" alt="" loading="lazy"><br>
安装完成后点击左下角类似“&gt;&lt;”的图表并选择Remote-WSL，vscode会自动在Ubuntu内安装对应服务。启动后点击顶部的“Terminal”选项，新建一个窗口，就可以通过命令行来操作Ubuntu系统了。</p>
<p>现在，Linux开发的大门已经向你打开了，去拥抱这广阔的世界吧！</p>
]]></content>
    </entry>
</feed>